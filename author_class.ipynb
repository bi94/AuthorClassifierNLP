{"cells":[{"metadata":{"_uuid":"989b145e-a56d-4b09-9605-48473d74ad1c","_cell_guid":"d00fae60-ccfd-486f-8d05-cce28afd28c4","trusted":true},"cell_type":"markdown","source":"# POEMS AUTHOR CLASSIFIER BASED ON ARTIFICIAL NEURAL NETWORKS"},{"metadata":{"_uuid":"ab993ca4-c46d-4775-a0dc-5eb94fe823ce","_cell_guid":"28341289-adb2-4ef2-a995-c1759ce0f55e","trusted":true},"cell_type":"markdown","source":"## Data preprocessing phase"},{"metadata":{"_uuid":"ff07b2a9-4eb3-4270-964a-1cc8e5fbd850","_cell_guid":"6360e8cf-3c31-4de5-96ca-23f924f1ed9f","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import print_function\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv(\"/kaggle/input/dataset.csv\", header=0, error_bad_lines=False, delimiter='\\t')\n# Any results you write to the current directory are saved as output.","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"46109ba6-46fb-466a-870f-8a73705a4059","_cell_guid":"27caf29e-356b-45a5-953b-81cfb3dfc71c","trusted":true},"cell_type":"code","source":"import collections\ncount = collections.Counter(df['author'].values)\nprint('Most common authors:\\n')\nprint(count.most_common(10))\nprint('\\nTotal number of authors:\\n')\nprint(len(count))\n#print(len(df['author'].unique()))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"49abc843-532d-4e78-8a0b-ce8749bc9fb4","_cell_guid":"d1522f57-a15b-45c4-b79b-d658f38fab40","trusted":true},"cell_type":"code","source":"import nltk\ndf = df.reset_index(drop=True)\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\nMAX_SEQUENCE_LENGTH = df.stanzas.map(len).max() \nprint(\"MAX_SEQUENCE_LENGTH = \", MAX_SEQUENCE_LENGTH)\n\nMIN_SEQUENCE_LENGTH = df.stanzas.map(len).min() \nprint(\"MIN_SEQUENCE_LENGTH = \", MIN_SEQUENCE_LENGTH)\n\nMEAN_SEQUENCE_LENGTH = df.stanzas.map(len).mean() \nprint(\"MEAN_SEQUENCE_LENGTH = \", MEAN_SEQUENCE_LENGTH)\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text\ndf['stanzas'] = df['stanzas'].apply(clean_text)\ndf['stanzas'] = df['stanzas'].str.replace('\\d+', '')","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a8e0e518-6264-4473-a591-20ef8f1f4e72","_cell_guid":"487cb67b-bade-4513-997f-e8ea22c60b82","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nframe = {'len': df['stanzas'].astype(str).map(len)} \nresult = pd.DataFrame(frame)\n#print(result['len'].value_counts().max())\nhist = result.plot.hist(figsize = (20,10), bins = 30, range = [1, 2725], color='DarkGreen')\nhist.set_title('Stanzas length frequency', fontsize = 30)\nhist.set_xlabel('# of chars in stanza', fontsize = 20)\nhist.set_ylabel('Frequency', fontsize = 20)\nhist.legend(prop={'size': 20})","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"4208c27c-7d93-4223-839f-42662fc145a1","_cell_guid":"e3d78c27-91aa-4a3d-98e1-e93ec08d6d99","trusted":true},"cell_type":"code","source":"df = df.replace('? ?', np.nan)\ndf.dropna(inplace= True)\nvalue_counts = df['author'].value_counts()\nprint(value_counts)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"7221930e-6995-411c-9559-75cefd0f9b62","_cell_guid":"fc5d362c-8fe7-4818-a51f-e62808833e88","trusted":true},"cell_type":"code","source":"# Select the values where the count is less than a certain number\nto_remove = value_counts[value_counts < 500].index\n# Keep rows where the city column is not in to_remove\ndf = df[~df.author.isin(to_remove)]\ndf","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e8b13a64-cdc7-43c7-84f7-109028c5c3b5","_cell_guid":"269ce6f1-6d2a-4b8a-be15-1996248c7180","trusted":true},"cell_type":"markdown","source":"## Neural network models"},{"metadata":{"_uuid":"205c0d86-34ea-40b3-8c44-d31dc77ce5f2","_cell_guid":"b1c54dc5-d31f-4f21-957c-1b9386ce8433","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 40000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 200\n# This is fixed.\nEMBEDDING_DIM = 250\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['stanzas'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = tokenizer.texts_to_sequences(df['stanzas'].values)\nX = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nY = pd.get_dummies(df['author']).values\nprint('Shape of label tensor:', Y.shape)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 42)\nprint(\"Shape of training set = \", X_train.shape,Y_train.shape)\nprint(\"Shape of test set = \", X_test.shape,Y_test.shape)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f8047644-6610-4971-a0d2-cedbc6037c07","_cell_guid":"a9685731-f391-42e0-9e3d-17a8a058195d","trusted":true},"cell_type":"markdown","source":"### 1st model: bidirectional LSTM"},{"metadata":{"_uuid":"92a70086-58c4-40a8-8079-b6b4bb1bf905","_cell_guid":"72cc8b2b-e643-45e3-8888-2a37bedd23b2","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n#model.add(Dropout(0.2))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"cf7a4fb5-0646-4fb1-9d8f-628277179e19","_cell_guid":"4c399b0a-1ef4-4bda-b36a-582b5db36c28","trusted":true},"cell_type":"code","source":"model.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(Y_train.shape[1], activation='sigmoid'))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"97981802-b754-4801-8664-7f035888759d","_cell_guid":"87c9d173-27c3-4112-a331-f2429ae167c9","trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nepochs = 4\nbatch_size = 32\n\n#history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel.save_weights(\"model1.h5\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"29b74717-10e8-481c-8da2-59848c9d0ed1","_cell_guid":"48cf5a5e-911e-4ada-9b3e-426950fe8533","trusted":true},"cell_type":"code","source":"# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history.history) \n\n# save to json:  \nhist_json_file = 'history1.json' \nwith open(hist_json_file, mode='w') as f:\n    hist_df.to_json(f)\n\n# or save to csv: \nhist_csv_file = 'history1.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a43de57c-fd4c-4fb6-8b99-5417c5b81206","_cell_guid":"0f256069-9b67-4b18-8754-cdffa8ca2194","trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('accuracy.png')\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('loss.png')","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"028b3e98-1084-4902-8645-7743e20493f8","_cell_guid":"a2efb1cc-d89f-4c8d-a4a8-00658a74d41f","trusted":true},"cell_type":"markdown","source":"### 2nd model: standard LSTM"},{"metadata":{"_uuid":"bc0da036-de84-4182-8703-c6384999711e","_cell_guid":"2daac167-6969-4a82-995e-4c3e658440e9","trusted":true},"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel3.add(Dropout(0.2))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a370d113-6d38-41bf-add5-641b5c45479f","_cell_guid":"445a6deb-ac9f-4492-acd9-5aa496d660b5","trusted":true},"cell_type":"code","source":"model3.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel3.add(Dense(Y_train.shape[1], activation='softmax'))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d1a222d6-9b2f-447b-80cb-78b50afe99a3","_cell_guid":"d2aac072-4200-4115-9bc1-455e42e0feb1","trusted":true},"cell_type":"code","source":"model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel3.summary()\nepochs = 3\nbatch_size = 64\n\n#history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\nhistory3 = model3.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel3.save_weights(\"model3.h5\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"7150e4eb-27f5-4463-95ba-aad0c58da62f","_cell_guid":"3b75491e-e8b2-4630-9d9d-13a041daa928","trusted":true},"cell_type":"code","source":"# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history3.history) \n\n# save to json:  \nhist_json_file = 'history3.json' \nwith open(hist_json_file, mode='w') as f:\n    hist_df.to_json(f)\n\n# or save to csv: \nhist_csv_file = 'history3.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ec35ee2e-c730-4531-9b13-b1869cf896ad","_cell_guid":"e27d3813-d00b-4756-9307-1834105e1d40","trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history3.history['accuracy'])\nplt.plot(history3.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('accuracy.png')\n\n# Plot training & validation loss values\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('loss.png')","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a7694e9b-672d-4062-aecd-b946630a583e","_cell_guid":"17caa5dc-45ae-4570-9925-43bedb195c22","trusted":true},"cell_type":"markdown","source":"### 3rd model: CNN"},{"metadata":{"_uuid":"497741b4-c0da-4354-bd12-103679ef18b6","_cell_guid":"c58d2216-5178-4772-9fa9-28b97782b2c1","trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel2.add(Dropout(0.2))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"9be86ac8-126b-487b-bb7c-189bb19c3282","_cell_guid":"0501fbcc-930e-46b3-b912-b0f44bd3d3e8","trusted":true},"cell_type":"code","source":"filters = 250\nkernel_size = 3\nhidden_dims = 250\nmodel2.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\nmodel2.add(GlobalMaxPooling1D())\nmodel2.add(Dense(hidden_dims))\nmodel2.add(Dropout(0.2))\nmodel2.add(Activation('relu'))\nmodel2.add(Dense(Y_test.shape[1]))\nmodel2.add(Activation('sigmoid'))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"39d2c309-68f5-4536-9271-4c37ab022b3e","_cell_guid":"43bd76d7-4d41-465a-87fa-740df8c884f0","trusted":true},"cell_type":"code","source":"model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel2.summary()\nepochs = 3\nbatch_size = 32\n\nhistory2 = model2.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n#history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel.save_weights(\"model2.h5\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"639b299a-4544-4a31-b3f0-43e1e054d10e","_cell_guid":"3b440754-5283-4c59-bf4d-73ad68052ae0","trusted":true},"cell_type":"code","source":"# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history2.history) \n\n# save to json:  \nhist_json_file = 'history2.json' \nwith open(hist_json_file, mode='w') as f:\n    hist_df.to_json(f)\n\n# or save to csv: \nhist_csv_file = 'history2.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"58de9133-e426-4713-ac73-76c61520e8c1","_cell_guid":"b88aacbc-0957-4448-91cf-a6ccff18020d","trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('accuracy2.png')\n\n# Plot training & validation loss values\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\nplt.savefig('loss2.png')","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}